# Natural Language Processing

### 자연어 처리 (Natural Language Processing)
- 자연어란 사람과 사람이 일상 생활에서 서로 대화하는데 사용하는 언어를 뜻하며, 이러한 자연어의 의미를 분석하여 컴퓨터가 처리 할 수 있도록 하는 일 
- 자연어 처리는 전처리를 위한 형태소 분석, 개체명 인식, 구문 분석부터 음성인식, 내용 요약, 번역, 사용자의 감성 분석, 텍스트분류작업 (스팸, 뉴스 분류), 질의 응답 시스템, 챗봇 등의 응용분야에 활용이 된다.

### 자연어 처리 task
- Part-of-Speech 태깅
- 형태소 분석(한글)
- 개체명 인식 
- 구문 분석 
- 상호 참조 
- 감정 분석 
- 번역 
- 질의 응답 
- 기계 독해 
- 텍스트 생성 
- 텍스트 요약 
- 대화 시스템(챗봇)
- 언어 모델

### 형태소 분석 
- 형태소의 정의
    - 의미가 있는 최소 단위
    - 문법적, 관계적은 뜻을 나타내는 단어 또는 단어의 부분
- 형태소 분석
    - 단어를 구성하는 각 형태소를 분리하고 기본형 및 품사 정보를 추출

### 개체명 인식 
- 개체명의 정의
    - 개체명이란 사람이름, 회사이름, 지명, 영화제목, 날짜, 시간 등을 말함
- 개체명 인식
    - 개체명 인식이란 텍스트에서 개체명을 찾아서 태깅하는 것을 개체명 인식이라고 함

### 구문 분석
- 구문 분석은 문장을 이루고 있는 구성 성분으로 분해하고, 위계 관계를 분석하여 문장의 구조를 결정하는 것

### 지도학습, 비지도학습, 강화학습 차이점을 서술하시오.
- 지도 학습(Supervised Learning) : 레이블(Label)이라는 정답과 함께 학습하는 것 (ex. 분류)
  - 정답의 데이터가 존재하는 상황에서 학습하는 알고리즘이며, 좀 더 얼밀하게 정의하면, 입력데이터 x와 그에 대한 정답 레이블 y의 쌍(x, y)를 이용해서 학습하는 알고리즘 
- 비지도 학습(Unsupervised Learning) : 레이블이 없이 학습하는 것 (ex. 워드투벡터(word2Vec))
  - 정답 레이블 y 없이 입력데이터 x만을 이용해서 학습하는 알고리즘, 지도 학습의 목적이 어떤 값에 대한 예측을 수행하는 것이라면, 비지도 학스은 데이터의 숨겨진 특징(Hidden Featrue)를 찾아내는 것에 목적이 있다 
- 강화학습(Reinforcement Learning) : 행동(action)에 대한 보상(rewards) 기반으로 학습 (ex. 주로 게임에서 활용)
  - 강화 학습은 에이전트 (Agent)가 주어진 환경(State)에서 어떤 행동을 취하고 이에 대한 보상(Reward)을 얻으면서 학습을 진행

### 단순 선형 회귀 분석의 loss function에 대하여 서술하시오.
- 단순 선형 회귀 분석은 하나의 종속 변수에 대해 독립변수가 하나인 경우이며, 𝐻 𝑥 = 𝑊𝑥 + b(𝑥: 독립변수, 𝑊: 가중치(weight), 𝑏 : 편향(bias))로 사용한다.
  이때, loss function(cost function)은 가설과 실제 데이터를 넣었을 때의 값의 차이를 설명한 것이며, 학습의 차이를 줄이는 방향으로 가중치(W)와 편향(b)을 조정한다.
  하지만 차이에 대한 실제값과 이론값의 합이 정확한 값이 나오지 않는 경우가 발생할 수 있기 때문에 절대값과 제곱을 사용하여 문제를 해결하며, 여기서 제곱을 사용하면
  가중치가 높아지는 효과가 발생하며, 이것을 위해서 평균 제곱 오차(MSE: Mean Squared Error)를 사용하여
  임의의 직선을 그어 이에 대한 평균 제곱 오차를 구하고, 이 값(오차)을 가장 작게 만들어주는 값을 찾아 가는 작업 한다.

### gradient vanishing problem에 대해서 서술하시오.
- Sigmoid 나 tanh와 같은 활성화 함수(activation function)가 역전파 과정에서 0에 가까운 아주 작은 기울기가 계속 곱해지면, 
  앞 단에는 기울기가 잘 전달되지 않게 되며, 출력이 점점 작아지는 진다.
- 네트워크가 깊어질수록 더욱 심각해지는 현상이 발생하며, 이를 완화하기 위해서 ReLU와 같은 활성화 함수를 이용한다 
- ReLU는 양수값에선 특정 값에 수렴하지 않아서 시그모이드 함수나 하이퍼볼릭탄젠트에 비해 더 잘 동작하며, 연산도 간단하여 속도도 훨씬 빠름
  하지만,  입력값이 음수이면 기울기가 0이 되어, 뉴런이 회생할 수 없다는 뜻의 죽은 렐루(dying ReLU)현상이 발생하기 때문에 
  입력값이 음수 일땐, 매우 작게 값을 변경하는 리키 함수(Leaky ReLU)함수를 이용한다. 

### learning rate와 weight decay에 대해서 서술하시오.
- 경사 하강법(Gradient Descent)은 임의의 W값을 정한 뒤에, cost가 최소가 되도록 W를 조금씩 수정하는 방법이며, 미분을 통한 접선에서의 기울기를 활용한다
즉, 경사하강법은 오차 변화에 따라 이차 함수 그래프를 만들고 적절한 학습률을 설절해 미분 값이 0인 지점을 구하는 것이다.  
여기서 사용하는 학습률(𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔 𝑟𝑎𝑡e)을 통해서 적절한 조절하여 사용하게 되는데, W의 값을 변경할 때, 얼마나 크게 변경할지를 결정한다
 학습률을 너무 크게 정하면 발산하게 되고, 너무 작게 정하면 학습 속도가 느려질수 있다.

### BERT의 학습 방법에 대하여 서술하시오.
: BERT는 Transformer의 인코더를 적층한 모델을 사용하며, 잘 만들어진 BERT 언어모델 위에 1개의 classification layer만 부착하여 다양한 NLP task를 수행한다
 영어권에서 11개의 NLP task에 대해 state-of-the-art (SOTA) 달성하며 당시에는 모든 부분에서 최상의 조건을 달성하였다.
 
- MLM(Masked Language Model)
  - 입력 문장에서 임의로 토큰을 masking 한 후에, 해당 토큰을 맞추는 학습
- NSP(Next Sentence Prediction)
  - 두 문장이 주어졌을 때, 두 문장의 순서를 예측하는 방식

### Universal Sentence Encoder의 단점에 대해서 아는대로 서술하시오.
- 성능은 좋으나 파인튜닝이나 연장학습이 있어야 한다. 
    - 파인 튜닝(fine-truning) : 기존에 학습되어져 있는 모델을 기반으로 아키텍쳐를 새로운 목적(나의 이미지 데이터에 맞게)변형하고 이미 학습된 모델 Weights로 부터 학습을 업데이트하는 방법